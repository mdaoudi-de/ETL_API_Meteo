# ETL-API-Meteo ğŸŒ¦ï¸  
ETL Â« API â†’ Parquet partitionnÃ© â†’ SQLite Â» pour des donnÃ©es mÃ©tÃ©o quotidiennes.

## 1. Objectif du projet

Construire un petit pipeline Data Engineering de bout en bout :

- interroger lâ€™API **Open-Meteo** (prÃ©visions) et lâ€™API **ERA5** (archives)  
- normaliser les rÃ©ponses JSON en tableaux propres  
- stocker les donnÃ©es en **Parquet partitionnÃ©** (`date`, `city`) dans un mini *data lake* local  
- alimenter une base **SQLite** pour faire quelques requÃªtes analytiques de dÃ©mo  
- exÃ©cuter le tout via un **Makefile** ou une **CLI Python**.

Projet pensÃ© pour servir de **portfolio Data Engineer** (structure propre, bonnes pratiques, extensible vers Docker / Airflow / GCP).

---

## 2. Architecture technique

### 2.1. Stack utilisÃ©e

- **Python** (3.11)
  - `requests` â€“ appels API
  - `pandas` â€“ normalisation tabulaire
  - `pyarrow` / `parquet` â€“ Ã©criture Parquet
  - `duckdb` â€“ lecture globale des Parquet
  - `sqlite3` â€“ base analytique locale
- **Format de stockage**
  - JSONL brut (ingestion)
  - CSV (staging)
  - **Parquet** partitionnÃ© (`date`, `city`)
  - SQLite (`data/warehouse/weather.sqlite`)
- **Outils**
  - `make` â€“ orchestration simple
  - (optionnel) `pre-commit`, `black`, `flake8`

---

## 3. Arborescence du projet

```text
ETL-API-Meteo/
â”œâ”€â”€ .github/workflows/ci.yml          # CI GitHub (tests / lint Ã  complÃ©ter)
â”œâ”€â”€ airflow/                          # futur DAG Airflow (optionnel)
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ streamlit_dashboard/          # futur dashboard Streamlit (optionnel)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ cities.csv                    # ville, latitude, longitude
â”‚   â”œâ”€â”€ config.yaml                   # config gÃ©nÃ©rique (optionnel)
â”‚   â””â”€â”€ logging.yaml                  # config logs (optionnel)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ forecast/                 # JSONL bruts (API forecast)
â”‚   â”‚   â””â”€â”€ archive/                  # JSONL bruts (API archives ERA5)
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ forecast/                 # CSV normalisÃ©s
â”‚   â”‚   â””â”€â”€ archive/
â”‚   â”œâ”€â”€ parquet/                      # data lake local Parquet (date/city)
â”‚   â””â”€â”€ warehouse/
â”‚       â””â”€â”€ weather.sqlite            # base SQLite analytique
â”œâ”€â”€ docker/                           # Dockerfile / compose (Ã  complÃ©ter)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl_weather/
â”‚       â”œâ”€â”€ ingest/
â”‚       â”‚   â”œâ”€â”€ open_meteo_client.py  # client API gÃ©nÃ©rique
â”‚       â”‚   â”œâ”€â”€ fetch_forecast.py     # ingestion forecast multi-villes
â”‚       â”‚   â””â”€â”€ fetch_archive.py      # ingestion archives multi-villes
â”‚       â”œâ”€â”€ transform/
â”‚       â”‚   â”œâ”€â”€ normalize_forecast.py # JSON â†’ CSV (forecast)
â”‚       â”‚   â””â”€â”€ normalize_archive.py  # JSON â†’ CSV (archives)
â”‚       â”œâ”€â”€ load/
â”‚       â”‚   â”œâ”€â”€ to_parquet.py         # CSV â†’ Parquet partitionnÃ©
â”‚       â”‚   â””â”€â”€ to_sqlite.py          # Parquet â†’ SQLite + requÃªtes de dÃ©mo
â”‚       â””â”€â”€ cli.py                    # pipeline complet en 1 commande
â”œâ”€â”€ tests/                            # tests unitaires (Ã  complÃ©ter)
â”œâ”€â”€ Makefile                          # commandes make (pull / transform / load / all)
â”œâ”€â”€ pyproject.toml                    # config projet / pytest
â”œâ”€â”€ requirements.txt                  # dÃ©pendances Python
â””â”€â”€ README.md  
```

---

## 4. DonnÃ©es manipulÃ©es

### 4.1 Sources de donnÃ©es

- API Open-Meteo Forecast : https://api.open-meteo.com/v1/forecast  
- API Open-Meteo ERA5 Archive : https://archive-api.open-meteo.com/v1/era5  
- CoordonnÃ©es des villes dans `config/cities.csv`

### 4.2 Variables mÃ©tÃ©o utilisÃ©es

Le pipeline utilise principalement les champs suivants :

- temperature_2m_max : tempÃ©rature maximale quotidienne
- temperature_2m_min : tempÃ©rature minimale quotidienne
- precipitation_sum : cumul des prÃ©cipitations
- time : date du jour
- city : nom de la ville
- source : "forecast" ou "archive"

### 4.3 Format des donnÃ©es

- Ã‰tape ingestion : JSONL bruts
- Ã‰tape staging : CSV normalisÃ©s
- Ã‰tape data lake : Parquet partitionnÃ©
- Ã‰tape analyse : SQLite

---

## 5. Data Lake local (Parquet)

Les donnÃ©es normalisÃ©es sont stockÃ©es en Parquet avec un partitionnement :

```text
data/parquet/
â””â”€â”€ date=YYYY-MM-DD/
â””â”€â”€ city=nom-ville/
â””â”€â”€ part-0000.parquet
```


Chaque fichier contient :

- date : string
- city : string
- temperature_2m_max : float
- temperature_2m_min : float
- precipitation_sum : float
- source : forecast ou archive

Le format Parquet permet :

- compression importante
- lecture rapide colonne par colonne
- compatibilitÃ© directe avec Spark, DuckDB, BigQuery, etc.
- filtrage efficace par partitions (date, ville)

---

## 6. Installation

### 6.1 Clonage du dÃ©pÃ´t

git clone <URL_DU_REPO>
cd ETL-API-Meteo


### 6.2 CrÃ©ation de l'environnement

python -m venv .venv


Activation :

- Windows :
.venv\Scripts\activate


- Linux / macOS :
source .venv/bin/activate


### 6.3 Installation des dÃ©pendances
pip install -r requirements.txt

Ou :
make install


---

## 7. ExÃ©cution du pipeline

Le pipeline suit les Ã©tapes suivantes :

1. ingestion (API â†’ JSONL)
2. transformation (JSONL â†’ CSV)
3. chargement Parquet
4. chargement SQLite

### 7.1 ExÃ©cution via Makefile

make pull
make transform
make load
make sqlite
make etl
make all

### 7.2 ExÃ©cution via la CLI Python

python -m src.etl_weather.cli

Pour charger dans SQLite :
python -m etl_weather.load.to_sqlite

## 8. EntrepÃ´t SQLite

La base SQLite est enregistrÃ©e ici :
data/warehouse/weather.sqlite

Elle contient une table :
weather(date, city, temperature_2m_max, temperature_2m_min, precipitation_sum)

Exemples de requÃªtes exÃ©cutÃ©es automatiquement :

### 1) Top 5 des jours les plus chauds

SELECT date, city, temperature_2m_max
FROM weather
ORDER BY temperature_2m_max DESC
LIMIT 5;

### 2) Moyenne pluie par ville

SELECT city, AVG(precipitation_sum)
FROM weather
GROUP BY city
ORDER BY AVG(precipitation_sum) DESC;

### 3) TempÃ©rature max moyenne par ville

SELECT city, AVG(temperature_2m_max)
FROM weather
GROUP BY city
ORDER BY AVG(temperature_2m_max) DESC;


---

## 9. QualitÃ© et tests (Ã  complÃ©ter)

### 9.1 PrÃ©-commit (optionnel)

pip install pre-commit
pre-commit install
pre-commit run --all-files


### 9.2 Tests unitaires possibles

- test client API
- test normalisation
- test Ã©criture Parquet
- test partitions date/city

### 9.3 CI GitHub (optionnel)

Le fichier `.github/workflows/ci.yml` peut effectuer :

- installation Python
- pytest
- black --check
- flake8

---

## 10. Extensions possibles

- Dockerisation du pipeline
- Mini DAG Airflow
- Export du data lake sur Google Cloud Storage
- Table externe BigQuery sur le bucket GCS
- Dashboard Streamlit
- Validation des donnÃ©es (Pandera / Great Expectations)

---

## 11. RÃ©sumÃ©

Ce projet implÃ©mente un pipeline complet :

API â†’ JSONL â†’ CSV â†’ Parquet partitionnÃ© â†’ SQLite.

Il suit une architecture professionnelle et extensible.
